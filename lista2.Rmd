---
output: html_document
title: ' '
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = FALSE)
```

<style>
  body{
    text-align: justify;
    }

</style>

<h3 style="text-align: center;">UNIVERSIDADE ESTADUAL DE MARINGÁ <br> CURSO DE CIÊNCIAS EXATAS <br> CURSO DE ESTATÍSTICA</h3>


<br><br><br><br><br><br><br><br>

<h3 style="text-align: center;">Lorival Coelho Junior</h3>

<br><br><br><br><br><br><br><br>

<h3 style="text-align: center;">Análise de Regressão - Lista 1</h3>

<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>

<h3 style="text-align: center;">Maringá <br> 2023</h3>
<div style="page-break-after: always;"></div>


## Lista de Exercícios II 

<br><br>

1. Seja $Y = X\beta + \epsilon,$ $\epsilon \sim N_n(0, \sigma^2I)$. Se $X_1$ e $X_2$ são variáveis independentes e $Y$ é a dependente de uma amostra aleatória com n = 30 elementos, tal que:

\begin{equation}
X'X =  \begin{bmatrix}
8&15&12 \\
15&33&24 \\
12&24&24 \\
\end{bmatrix};   

X'y = \begin{bmatrix}
57\\
121\\
99.5\\
\end{bmatrix};

y'y = 465.5

\end{equation}

<br><br>

(a) Determine a equação de regressão estimada considerando um MRLM do tipo: $Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \epsilon$;

Temos que $\hat \beta = (X'X)^{-1} X'Y$, como já temos X'X, invertendo-o,

```{r}
xx <- matrix(c(8,15,12,
               15,33,24,
               12,24,24), byrow = T, nrow = 3)

xy <- matrix(c(57,121,99.5),)

yy <- 464.5

inv <- round(solve(xx),4 )

betas <- inv%*%xy


```

\begin{equation}
(X'X)^{-1} =  \begin{bmatrix}
1.0000&-0.3333&-0.1667\\
-0.3333&0.2222&-0.0556\\
-0.1667&-0.0556&0.1806 \\
\end{bmatrix};
\end{equation}

Agora calculando a matriz $\beta$,

\begin{equation}
\beta =  \begin{bmatrix}
0.0840\\
2.3559\\
1.7402\\
\end{bmatrix};
\end{equation}

<br><br>

Assim a equação de regressão é $0.0840 + 2.3559x_1 + 1.7402x_2 + \epsilon$.

(b) Determine a $SQE = y'y - \hat \beta'X'y $ e aprensente a tabela de ANOVA;

```{r}
SQE   <- yy - t(betas) %*% xy
SQT   <- yy - yy/30
SQReg <- SQT - SQE

```

$SQE = y'y - \hat \beta'X'y = 464.5 - 463.0047 = 1.4953$

$SQT = y'y - \frac{y'y}{n} = 465.5 - 15.4833 = 449.0167$

$SQReg = SQT - SQE = 449.0167 - 1.4953 =  447.5213$

\hline
\begin{array}{ccccc}
\hline
\textrm{Causa de Variação}   & GL & SQ & QM F        \\
\hline
\textrm{Regressão} & 2 & 447.5213 & 223.7606& 4040.4586   \\
\textrm{Erro}      & 27& 1.4953   & 0.05538  & \\
\hline
\textrm{Total}     & 29& 449.0167 & 15.4833 & \\
\hline
\end{array}

<br><br>
(c) Realize o teste F de ajustamento global e conclua.

O teste possui as seguintes hipóteses,



  \begin{cases}
H_0: \beta_1 = \beta_2 = ... = \beta_p = 0\\
H_1: \textrm{nem todos os \beta_j (j = 1,2,...,p) são iguais a zero}
    \end{cases}


No qual testamos, se $F_{H_0} >F_{\alpha, p-1, n-p}$

Usando $\alpha = 0.05$, $F_{0.95, 2, 27}$, o $F_{H_0} = 4040$ é maior que o F tabelado. Assim, rejeitamos $H_0$

<br><br>

2. Analogamente ao exercício anterior, considere n = 10, p = 3, y'y = 56 e as equações normais:


  \begin{cases}
4\beta_1 + 2\beta_2 - 2\beta_3 ={ 4}\\
2\beta_1 + 2\beta_2 + 1\beta_3 = { 7}\\
-2\beta_1 + \beta_2 + 6\beta_3 ={ 9}
    \end{cases}


Encontre os estimadores de Mínimos Quadrados (EMQ) para $\beta$ e para $\sigma^2$.

<br>

Resolvendo o sistema encontramos,

$\beta_1 = 0, \beta_2 = 3, \beta_3 = 1$

<br>

E o estimador de $\hat \sigma^2 = \frac{Y'Y - \hat \beta'X'Y}{n- p} = QME$

Portanto,

```{r}
beta  <- matrix(c(0,3,1), nrow =1)

sigma <- (56 - beta %*% xy)/(10-3)
```

<br><br>

3. Para se estudar a influência das variáveis capital investido ($X_1$) e gasto em publicidade ($X_2$) no lucro anual ($Y$) de empresas, foram observadas essas variáveis em doze empresas em um mesmo ano. Os seguintes resultados foram registrados, na unidade de R$ 100 mil.

<br>

\begin{array}{c|cccccccc}
\hline
\textrm{Y}   &12&13&3&3&11&19&1&14&15&17&2&15       \\
\hline
\textrm{$X_1$} &31&16&29&19&27&21&24&11&26&18&12&3      \\
\textrm{$X_2$} &4&5&3&0&2&6&2&3&6&6&1&5                  \\
\hline
\end{array}
<br>

(a) Ajuste a estes dados um MRLM do tipo: $Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \epsilon$ por meio do MMQ e compare com as estimativas obtidas via comando *lm* do R;

$\hat \beta = (X'X)^{-1}X'Y$


```{r}
dados <- data.frame(y = c(12,13,3,3,11,19,1,14,15,17,2,15),
                    x1= c(31,16,29,19,27,21,24,11,26,18,12,3),
                    x2= c(4,5,3,0,2,6,2,3,6,6,1,5))



n <- nrow(dados)

xx <- matrix(c(n, sum(dados$x1), sum(dados$x2),
               sum(dados$x1), sum(dados$x1^2), sum(dados$x1 * dados$x2),
               sum(dados$x2), sum(dados$x1 * dados$x2), sum(dados$x2^2)), nrow = 3, byrow= 3)

print("Temos que X'X")
print(xx)

print("E que X'Y,")
print(xy)

xy <- matrix(c(sum(dados$y), sum(dados$x1 * dados$y), sum(dados$x2 * dados$y)))

beta <- solve(xx) %*% xy


print('Daí temos que os betas são,')

print(beta)
```
<br>

Agora comparando com o do *lm* do R

```{r, echo = TRUE}
lm(dados$y ~ dados$x1 + dados$x2)$coefficients
```
<br>

Os valores são identicos.

<br><br>


(b) Apresente graficamente: matriz de correlações múltiplas, o diagrama de dispersão e o ajuste no hiperplano;
<br>

A matriz de correlação múltipla é a seguinte,

```{r}
library(corrplot)

corrplot(cor(dados), method = 'color',
         type = 'lower'  ,
         addCoef.col ='black',tl.col="black",
         number.cex = 0.8, diag=T)

```

Temos que $Y$ tem correlação fraca com $X_1$ e forte com $X_2$, e $X_1$ não tem correlação com $X_2$.

<br>

O diagrama de dipersão com o ajuste do hiperplano é,

```{r}
library(scatterplot3d)
i = 15 # angulo de observacao
graph = scatterplot3d(dados$x1,dados$x2,dados$y,
                      angle=i, pch=19, cex.symbols=0.9,
                      color = 4,
                      xlab="Capital investido",
					            ylab="Gasto em publicidade",
                      zlab="Lucro anual")

# Plano estimado
reg <- lm(dados$y ~ dados$x1 + dados$x2)
graph$plane3d(reg, col="red")
```

<br><br>


(c) Realize a ANOVA, apresente o $R^2$ do ajuste e interprete o seu valor;

A tabela da ANOVA para o modelo foi a seguinte,
```{r}
anova(reg)
```

<br>
Nela $X_2$ teve um SQ e QM maior que $X_1$, explicando mais da variável resposta.

<br>

Quanto ao $R^2$,

```{r}
summary(reg)
```
Foi obtido um $R^2$ de 0.7395 e um ajustado de 0.6816, assim, o modelo está começando a explicar bem os dados (68%), no entanto, por ter poucos dados o modelo pode não ser representativo.

<br>


(d) Os dados fornecem evidências suficientes para indicar que o modelo contribui significativamente para a predição de Y para valores médios das variávies explicativas? Em caso afirmativo, apresente-o juntamente com seu IC;
<br>

Para isso, fazendo o teste de ajustamento global para verificar se os $\beta_j$ são significativos para explicar os dados.

```{r}
anova(lm(dados$y ~ dados$x1 + dados$x2))
```
Temos que $SQReg = 318.7 + 14.7 = 333.4$, $QMReg = \frac{333.4}{1}$, e o $QME = 13.1$, daí $F=\frac{333.4}{13.1}$

<br>

Enquanto o F tabelado, $F_{p-1,n-p} = 4.2564$

Assim, rejeitamos $H_0$.

```{r}
qf(.95, 2, 9)
```

(e) Teste as hipóteses $H_0: \beta_j$ versus $H_a: \beta_j \not = 0, j = 1,2$.

```{r}
summary(aov(dados$y ~ dados$x1 + dados$x2))
```

<br><br>

4. Um experimento foi realizado para estudar a relação entre o grau de corrosão de um certo metal, Y, e o tempo de exposição (em semanas), $X$, desse metal à ação da acidez do solo. Foram obtidos os seguintes resultados apresnetados na Tabela a seguir,


\begin{array}{c|cccccccc}
\hline
\textrm{X}   &1&2&3&4&5&6&7&8&9&10       \\
\hline
\textrm{Y}   0.08&0.18&0.32&0.53&0.88&1.30&1.95&2.80&3.90&4.60\\   
\hline
\end{array}

(a) Ajuste um MRLM a esses dados do tipo: $Y=\beta_0 + \beta_1X + \beta^2 + \epsilon$;

```{r}
dados <- data.frame(x = 1:10,
                    y = c(.08, .18, .32, .53, .88, 1.3, 1.95, 2.8, 3.9, 4.6))

dados$x2 <- dados$x^2

fit <- lm(y ~ x + x2, data = dados)
summary(fit)
```
Temos que o modelo estimado foi $Y = 0.274 -0.1987X + 0.0642X^2$.

<br><br>

(b) Apresente graficamente o ajuste;

```{r}
library(scatterplot3d)
i = 15 # angulo de observacao
graph = scatterplot3d(dados$x,dados$x2,dados$y,
                      angle=i, pch=19, cex.symbols=0.9,
                      color = 4,
                      xlab="Tempo de exposição (em semanas)",
					            ylab="Tempo (em semanas^2)",
                      zlab="Grau de corrosão")

# Plano estimado

graph$plane3d(fit, col="red")
```

<br><br>

(c) Realize a ANOVA;

```{r}
anova(fit)
```

Tanto $x$ quanto $x^2$ foram significativos, os resíduos tendo baixo QME comparando com o QMReg.

<br><br>

(d) Os dados fornecem evidências suficientes para indicar que o modelo contribui significativamente
para a predição de Y ?

Temos que o $R^2=0.9965$ e o ajustado $0.9955$, assim o modelo conseguindo explicar quase perfeitamente a variação. Olhando para os testes t, tanto a variável $x$ quanto $x^2$ são significativas ao nível de $\alpha = 0.05$. Além disso, a estatística $F=993.8$, assim rejeitando $H_0$ e pelo menos um dos $\beta_j$ é significativo.

<br><br>


(e) Teste as hipóteses $H_0 : \beta_1 = 0$ versus $H_a : \beta_1 \not = 0$;

```{r}
summary(fit)
```
$\beta_1$ foi significativo, tento $t_0 = -3.725$ e um $p=0.0074$, assim usando um $\alpha = 0.05$ rejeitamos $H_0$, portanto $\beta_1 \not = 0$

<br><br>

(f) Que porcentagem da variação total é explicada pelo modelo?

Olhando o $R^2$ que é 0.9965 e o ajustado 0.9955, assim temos que o modelo explica 99.55% da variação total.

<br><br>

(g) Apresente uma estimativa pontual e intervalar para $X_0 = 5.5$.

A estimativa pontual pro valor é
```{r}
xh       = 5.5 # valor novo
data.new = data.frame(x=xh, x2=xh^2)

predict(fit, data.new, se.fit=TRUE)$fit[[1]]
```
<br>

E o seu intervalo de confiança é o seguinte,

```{r}
pred.new = predict(fit, data.new, interval='confidence')
cat('[' ,pred.new[[2]], ',', pred.new[[3]], ']')
```



<br><br>

5. Para estudar a importância das variações do peso do ovo (PO) e do peso corporal (PC) sobre o consumo de alimentos (Y), em codornas de postura, foram anotados dados de 20 animais, apresentados
na Tabela abaixo.



\begin{array}{ccc}
\hline
Y&PO&PC\\
\hline
18.217&11.2&171.3\\
30.099&11.0&170.6\\
36.559&11.9&187.6\\
36.879&11.4&187.1\\
28.757&10.9&166.3\\
18.507&9.7&141.3\\
25.535&10.5&163.5\\
23.893&10.8&155.2\\
21.545&8.7&153.5\\
30.319&12.1&176.1\\
30.439&11.2&168.1\\
24.489&9.3&159.1\\
22.693&9.9&158.2\\
19.067&10.0&145.3\\
26.283&10.0&166.2\\
28.309&10.7&170.1\\
26.013&11.0&160.2\\
21.519&9.1&151.6\\
24.985&10.1&165.0\\
18.959&9.4&146.1\\
\hline
\end{array}


<br><br>

Apresente:

(a) As equações das RLS individuais (*Y versus PO* e *Y versus PC*) e os respectivos gráficos
ajustados;

```{r}

y  <- c(18.217, 30.099, 36.559, 36.879, 28.757,
        18.507, 25.535, 23.893, 21.545, 30.319,
        30.439, 24.489, 22.693, 19.067, 26.283,
        28.309, 26.013, 21.519, 24.985, 18.959)

po <- c(11.2, 11, 11.9, 11.4, 10.9,
        9.7, 10.5, 10.8, 8.7, 12.1,
        11.2, 9.3, 9.9, 10, 10,
        10.7, 11, 9.1, 10.1, 9.4)

pc <- c(171.3, 170.6, 187.6, 187.1, 166.3,
        141.3, 163.5, 155.2, 153.5, 176.1,
        168.1, 159.1, 158.2, 145.3, 166.2,
        170.1, 160.2, 151.6, 165.0, 146.1)


dados <- data.frame(y = y, po = po, pc = pc)
```

Olhando para o modelo $Y \sim PO$

```{r}
fit <- lm(y ~ po, data = dados)
summary(fit)

```

O modelo inicial foi $Y = -16.8290+4.0672\beta_1$, com o PO sendo significativo,  com o $R^2 = 0.4813$ e o ajustado $0.4525$, assim foi um modelo ruim.
<br>

Vendo o ajuste do modelo,

```{r}
plot(x = dados$po, y = dados$y,
     xlab = 'Peso do ovo (PO)',
     ylab = 'Consumo de alimentos (Y)',
     main = 'Gráfico de dispersão do consumo pelo PO')
curve(fit$coefficients[1] + fit$coefficients[2]*x ,  col = 'red', lwd = 2, add=T, lty=2)
```

O modelo passa no meio da maioria dos pontos, mas há vários longe da reta.

<br><br>

Quando ao  modelo $Y \sim PC$

```{r}
fit <- lm(y ~ pc, data = dados)
summary(fit)

```

O modelo inicial foi $Y = -36.0661 + 0.3783\beta_1$, com o intercepto e o PC sendo significativo,  com o $R^2 = 0.7507$ e o ajustado $0.7369$, assim foi um modelo melhor que o do PO. Com a estátistica F sendo significativa.
Vendo o ajuste do modelo,

```{r}
plot(x = dados$pc, y = dados$y,
     xlab = 'Peso corporal (PC)',
     ylab = 'Consumo de alimentos (Y)',
     main = 'Gráfico de dispersão do consumo pelo PC')
curve(fit$coefficients[1] + fit$coefficients[2]*x ,  col = 'red', lwd = 2, add=T, lty=2)
```

O ajuste ficou bem melhor e mais próximo da maioria dos pontos, tendo apenas um ponto (PC = 171) bem longe da reta.

<br><br>

(b) A equações das regressões quadráticas individuais (*PO versus Y* e *PC versus Y*) e decida pelo melhor ajuste, individualmente;

Olhando para o modelo $Y \sim PO^2$

```{r}
fit <- lm(y ~ I(po^2), data = dados)
summary(fit)
```

O modelo inicial foi $Y = 3.9522+0.1974\beta_1$, com o PO sendo significativo,  com o $R^2 = 0.4921$ e o ajustado $0.4639$, assim foi um modelo ruim. Além disso, a estatística F foi significativa.
<br>

Vendo o ajuste do modelo,

```{r}
plot(x = dados$po^2, y = dados$y,
     xlab = 'Peso do ovo (PO²)',
     ylab = 'Consumo de alimentos (Y)',
     main = 'Gráfico de dispersão do consumo pelo PO²')
curve(fit$coefficients[1] + fit$coefficients[2]*x ,  col = 'red', lwd = 2, add=T, lty=2)
```

O modelo passa no meio da maioria dos pontos, mas há vários longe da reta.

<br><br>

Quando ao  modelo $Y \sim PC$

```{r}
fit <- lm(y ~ I(pc^2), data = dados)
summary(fit)

```

O modelo inicial foi $Y = -5.2406 + 0.0011\beta_1$, com o $PC^2$ sendo significativo,  com o $R^2 = 0.758$ e o ajustado $0.7446$, assim foi um modelo melhor que o do $PO^2$. Com a estátistica F sendo significativa.
Vendo o ajuste do modelo,

```{r}
plot(x = dados$pc^2, y = dados$y,
     xlab = 'Peso corporal (PC²)',
     ylab = 'Consumo de alimentos (Y)',
     main = 'Gráfico de dispersão do consumo pelo PC²')
curve(fit$coefficients[1] + fit$coefficients[2]*x ,  col = 'red', lwd = 2, add=T, lty=2)
```

O modelo os pontos ficam mais próximos, tirando um ponto com PC próximo ao 29.000.

<br><br>

Graficamente os modelos são idênticos aos modelos$^1$, e o PC é um modelo melhor que o PO.

<br><br>

(c) A equação de uma RLM do tipo: $Y = PO + PC$;

```{r}
fit <- lm(y ~ po + pc, data = dados)
summary(fit)
```

Temos o modelo $Y = -36.3857 + 0.3147\cdot PO + 0.3601 \cdot PC$. Com o PO não sendo significativo, e o intercepto e o PC significativo com $\alpha < 0.001$. Quanto ao $R^2 =0.7519$ e o ajustado é 0.7227, assim o modelo explicando 72% da variação da resposta. Além disso, a estatística F é significativa.

<br><br>

(d) A equação de uma RLM do tipo: $Y = PO  + PO^2 + P C + PC^2 + PO \cdot PC$;

```{r}
fit <- lm(y ~ po + I(po^2) + pc + I(pc^2) + po*pc, data=dados)
summary(fit)
```
Usando esse modelo específico tivemos, $Y = 1.28 + 12.60PO -0.91PC + 1.51PO^2 + 0.01PC^2 -0.27PO\cdot PC$. No modelo, não tivemos nehnum fator significativo, quanto ao $R^2 = 0.7735$ e o ajustado $0.6926$, assim o modelo explicando, aproximadamente, 70% da variação. Quanto a estatística F foi significativa. No entanto, mesmo o $R^2$ sendo considerável e F sendo significativo, o modelo é muito saturado, explica menos que os com menos variáveis e, possivelmente, por uma variável ser função da outra deve haver multicolinearidade.

<br><br>

(e) Apresente o melhor modelo ajustado final, inclusive seu gráfico e conclua.

Usando o step

```{r}
library(MASS)
dados
fit <- lm(y ~ po + pc + po*pc, data =dados)
summary(fit)
fit0 <- stepAIC(fit, direction = 'backward')

summary(fit0)
```

O melhor modelo inicialmente foi $Y = -36.0661 + 0.3783\cdot PC$ tendo tanto o intercepto quanto ao PC foram significativos a um nível $\alpha < 0.001$. O AIC foi de 43.17, já o $R^2=0.7507$ quanto ao ajustado $0.7369$, sendo um $R^2$ maior que o dos modelos anteriores.

<br><br>


```{r}
plot(x = dados$pc, y = dados$y,
     xlab = 'Peso corporal (PC)',
     ylab = 'Consumo de alimentos (Y)',
     main = 'Gráfico de dispersão do consumo pelo PC')
curve(fit0$coefficients[1] + fit0$coefficients[2]*x ,  col = 'red', lwd = 2, add=T, lty=2)
```

O ajuste ficou próximo da maioria dos pontos, tendo apenas um ponto (PC = 171) bem longe da reta.

<br><br>

Agora analisando os pressupostos,

<br>

**Normalidade dos resíduos** <br>

Para isso, será usado o teste de Shapiro-Wilk, que possui as seguintes hipóteses,

$$
    \begin{cases}
H_{0}: \textrm{Os dados seguem uma distribuição normal.}\\
H_{1}: \textrm{Os dados não se distribuem como uma normal.}
    \end{cases}
$$
<br>
```{r}
shapiro.test(fit0$residuals)
```
Assim, utilizando 95% de confiança, rejeitamos $H_0$, temos que os resíduos não seguem uma distribuição normal.
<br>

Assim, os pressupostos do modelo não foram atendidos.

<br>

Olhando para os pontos buscando influentes,

```{r}
res = residuals(fit0) # residuos usuais
R   = rstandard(fit0) # residuos estandardizados
Ti   = rstudent(fit0)  # residuos rstudent
D   = cooks.distance(fit0)
h   = hatvalues(fit0) # leverage

hl  = influence(fit0) # Mostra varias medidas
HL  = influence.measures(fit0)

# Valores atipicos   se: |R,T|>3
# Valores influentes se: |D|>0.5
# Valores alavanca   se: |h|>0.5
influence.measures(fit0)
Residuos = round(cbind(Ti, R, D, h), 3)
Residuos              # i=1 -> atipico


```

Retirando o primeiro valor do banco e reajustando o modelo,

```{r}
fit1 <- lm(y ~ pc, data =dados[-1])
summary(fit1)

shapiro.test(fit1$residuals)
```
Esse continua sem normalidade dos resíduos.

<br><br>

O único modelo que atendeu aos pressupostos foi o $Y \sim PO$, mas esse teve um $R^2$ muito baixo, por isso será tentado tirar um valor influente.
```{r}
library(car)
library(lmtest)
fit3 <- lm(I(y) ~ po , data = dados)
#summary(fit3)

shapiro.test(fit3$residuals)
gqtest(fit3)
durbinWatsonTest(fit3)
```

```{r}
res = residuals(fit3) # residuos usuais
R   = rstandard(fit3) # residuos estandardizados
Ti   = rstudent(fit3)  # residuos rstudent
D   = cooks.distance(fit3)
h   = hatvalues(fit3) # leverage

hl  = influence(fit3) # Mostra varias medidas
HL  = influence.measures(fit3)

# Valores atipicos   se: |R,T|>3
# Valores influentes se: |D|>0.5
# Valores alavanca   se: |h|>0.5

Residuos = round(cbind(Ti, R, D, h), 3)
Residuos              # i=1 -> atipico
```

<br><br>

6. Um CEO de um grande banco deseja estudar a relação entre os salários mensais (Y : salários mínimos) e o tempo de experiência (E: anos) no cargo de gerente das agências. Outro interesse é
saber se existem diferenças quando são levados em conta os salários de homens e de mulheres. Para tal, sugere-se um MRLM do tipo: $Y = \beta_0 + \beta_1E + \beta_2S + \beta_3ES + \epsilon$, sendo S: sexo (0 se mulher e 1 se homem).
Considerando os dados coletados a seguir, apresente um relatório de análise estatística a este CEO:


\begin{array}{ccc}
\hline
Mulher: &Y&1,93&3,18&2,28&3,13&2,78&3,09&2,65&2,22&2,85&3,23&2,82&1,91\\
&E&0&17&5&15&9&15&8&5&13&20&11&1\\
\hline
Homem:&Y&2,54&2,57&4,22&4,09&3,61&4,71&3,15&2,99&4,75&4,12&2,36&4,09&4,51\\
&E&6&7&23&20&18&27&11&10&29&23&4&22&25\\
\hline
\end{array}

<br>

```{r}
dados <- data.frame(
  y = c(1.93,3.18,2.28,3.13,2.78,3.09,2.65,2.22,2.85,3.23,2.82,1.91,2.54,2.57,4.22,4.09,3.61,4.71,3.15,2.99,4.75,4.12,2.36,4.09,4.51),
  e = c(0,17,5,15,9,15,8,5,13,20,11,1,6,7,23,20,18,27,11,10,29,23,4,22,25),
  s = as.factor(c(rep(0, 12), rep(1, 13)))
)

fit <- lm(y ~ e + s+ e*s, data = dados)

summary(fit)
```
Usando o modelo sugerido tivemos $1.9451 + 0.0733E + 0.0166S_1 + 0.0002\cdot S_1 \cdot E$. Usando como base o Sexo = 0 (Feminino), o $R^2=0.9881$ e ajustado $0.9864$, ou seja, o modelo explicou quase toda variação. O intercepto, os anos de experiência (E) e a iteração experiência e sexo, o sexo sozinho não foi significativo. Além disso a estatística F foi significativa.

<br><br>

#### **Testando os pressupostos**

<br><br>
**Não multicolinearidade**

Para isso, será usado Fator de Influência  de Variância (VIF), que é calculado por

$$VIF_j = \frac{1}{1-R^2_j}$$
E é usado como valor crítico o 10, caso o VIF seja maior ou igual há multicolinearidade.

```{r, echo=T}
vif(fit)
```
Assim o vif das variáveis foi menor que 10, no entanto, o da iteração e:s foi bem alto quase chegando ao valor crítico, a iteração será mantida devido esse ser o modelo sugerido.
<br><br>

**Normalidade dos resíduos** <br>

Para isso, será usado o teste de Shapiro-Wilk, que possui as seguintes hipóteses,

$$
    \begin{cases}
H_{0}: \textrm{Os dados seguem uma distribuição normal.}\\
H_{1}: \textrm{Os dados não se distribuem como uma normal.}
    \end{cases}
$$
<br>
```{r}
shapiro.test(fit$residuals)
```
Assim, utilizando 95% de confiança, não rejeitamos $H_0$, temos que os resíduos seguem uma distribuição normal.
<br><br>

**Homocedastidade**

Para isso, o teste de Goldfeld-Quandt será usado, o qual possui as seguintes hipóteses,

$$
    \begin{cases}
H_{0}: \textrm{A homocedasticidade está presente.}\\
H_{1}: \textrm{A homocedasticidade não está presente.}
    \end{cases}
$$

```{r}
library(lmtest)

gqtest(fit)
```
Usando 95% de confiança não rejeitamos $H_0$, ou seja, a homocedasticidade está presente nas variáveis independentes.
<br><br>

**Resíduos independentes e não correlacionados**<br>
<br>


Para testar se existe de fato não correlação será usado o teste de Durbin-Watson, que possui as seguintes hipóteses,

$$
    \begin{cases}
H_{0}: \rho = 0,    &&\textrm{Existe independência}\\
H_{1}: \rho \not= 0,&&\textrm{Não existe independência}
    \end{cases}
$$

```{r}
library(car)
durbinWatsonTest(fit)
```
<br>
Assim, usando 95% de confiança, não rejeitamos $H_0$, ou seja, existe independência nos resíduos.
<br><br>

Assim, todos pressupostos foram aceitos o modelo sendo válido. Enfim, temos o modelo $Y = 1.9451 + 0.0733E + 0.0166S_1 + 0.0002\cdot S_1 \cdot E$, no qual o sexo não foi significativo, mas seu efeito foi de +0.01666 pra homens, enquanto a interação de ser homem e ano foi de +0.0002. Assim pelo modelo, homens ganham 0.02 salários mínimos a mais e também 0.0002 a mais a cada ano de experiência.

<br><br>

7. Foi realizado um experimento envolvendo 4 variedades de trigo plantadas em canteiros de mesmo tamanho, tendo sido observadas as seguintes variáveis:

Prod: produção de trigo (kg);

Prec: precipitação pluviométrica (cm);

Var: variedade (1, 2, 3 e 4);

Fert: concentração de fertilizante empregado na plantação (1, 2 e 3).

\begin{array}{cccc}
\hline
Prod&Prec&Var&Fert\\
\hline
15,6&14,4&1&1\\
16,2&13,2&1&1\\
17,1&14,8&1&2\\
16,7&13,6&1&2\\
15,1&14,0&1&3\\
17,1&12,8&1&3\\
18,3&14,4&2&1\\
17,2&13,2&2&1\\
17,4&14,8&2&2\\
18,4&13,6&2&2\\
19,7&14,0&2&3\\
17,6&12,8&2&3\\
\hline
\end{array}

\begin{array}{cccc}
\hline
Prod&Prec&Var&Fert\\
\hline
14,8&14,4&3&1\\
13,8&13,2&3&1\\
14,5&14,8&3&2\\
14,4&13,6&3&2\\
15,2&14,0&3&3\\
14,8&12,8&3&3\\
15,1&14,4&4&1\\
16,0&13,2&4&1\\
16,2&14,8&4&2\\
16,8&13,6&4&2\\
17,6&14,0&4&3\\
18,6&12,8&4&3\\
\hline
\end{array}

(a) Ajuste um modelo aos dados, considerando todas regressões possíveis (ver Exemplo disponível na página 33 no Material Didático de Giolo (Apostila-2007) - Análise de Regressão Linear Múltipla. Utilize todos os critérios para a comparação dos modelos;

```{r}
prod  <- c(15.6,16.2,17.1,16.7,15.1,17.1,18.3,17.2,17.4,18.4,19.7,17.6,14.8,13.8,14.5,14.4,15.2,14.8,15.1,16,16.2,16.8,17.6,18.6)

prec  <- c(rep(c(14.4,13.2,14.8,13.6,14,12.8), 4))

VAR   <- sort(rep(1:4, 6))

fert  <- rep(c(1,1,2,2,3,3), 4)

dados <- data.frame(prod = prod,
           prec = prec,
           var  = VAR,
           fert = fert)

```


(b) Ajuste um modelo, selecionando as variáveis significantes, que explique a produção de trigo, utilizando cada um dos métodos vistos (backward, forward e stepwise);

(c) Qual ajuste é melhor? Por que?

<br><br>

8. Utilize o banco de dados mtcars do R e faça uma análise em um MRLM completa considerando como resposta a variável mpg e todas as demais como regressoras sem considerar interações.

Os primeiros dados do banco são os seguintes,
```{r}
dados <- mtcars
head(dados)

dados$vs <- as.factor(dados$vs)
dados$am <- as.factor(dados$am)
```

<br>
Verificando a matriz de correlação,

```{r}
library(corrplot)

corrplot(cor(dados[,1:7]), method = 'color',
         type = 'lower'  ,
         addCoef.col ='black',tl.col="black",
         number.cex = 0.8, diag=F)
```
Olhando a variável resposta, temos correlação negativa forte com *cyl*, *disp*, *hp* e *wt*, também tem correlação forte com *drat*, e moderada com *qseq*. Além disso, há correlação entre muitas as variáveis explicativas, destacando as fortes, tanto negativas quanto positivas, há de *cyl* com *disp*, *hp*, *drat* e *wt*, *disp* com *hp*, *drat*, *wt*, de *hp* com *wt* e *qsec* e de *drat* com *wt*. Ou seja, é bem possível que haja multicolinearidade no modelo completo.
<br><br>


Ajustando um modelo completo,

```{r}
fit <- lm(mpg ~ ., data =dados)
summary(fit)
```
Usando todas as demais como regressão, nenhuma variável foi significativa a um $\alpha=0.05$, se considerar a confiança como 10%, *wt* seria. Quanto ao $R^2$ foi $0.869$ e o ajustado $0.8066$ sendo até alto, a estatística F foi significativa. Assim, possivelmente, há um modelo menos saturado que seja igual ou melhor.

<br><br>

Inicialmente testando a multicolinearidade para verificar o que foi visto na matriz de correlação.
Para isso, será usado Fator de Influência  de Variância (VIF), que é calculado por

$$VIF_j = \frac{1}{1-R^2_j}$$
E é usado como valor crítico o 10, caso o VIF seja maior ou igual há multicolinearidade.

```{r, echo=T}
vif(fit)
```
Assim o vif das variáveis *cyl*, *disp* e *wt* foram maiores que 10, furando o pressuposto. <br><br>

Será tentatdo usar o StepAIC para encontrar um modelo melhor.

```{r}
fit1 <- stepAIC(fit, direction = 'backward')
```

O modelo escolhido foi $\text{mpg} \sim \text{wt + qseq + am}$, com $AIC=61.31$.

<br><br>

```{r}
summary(fit1)
fit2 <- lm(mpg ~ wt + qsec + am, data = dados)

```

O modelo escolhido teve as variáveis *wt*, *qsec*, *am = 1*, foram significativos com $\alpha = 0.05$, o intercepto não. Quanto ao $R^2$ foi de 0.8497 e o ajustado 0.8336,  o ajuste explicando mais de 80% da variação. Além disso, a estatística F foi significativa.

<br><br>

```{r}
library(MASS)

w = boxcox(mpg ~ disp + hp + wt + qsec + am, data = dados, plotit=F)

posicao = which.max(w$y)

lambda = w$x[posicao]
dados$y2 = log(dados$mpg)
fit2 <- lm(y2 ~ disp + hp + wt + qsec + am, data = dados)

```



#### Testando os Pressupostos
**Homocedastidade**

Para isso, o teste de Goldfeld-Quandt será usado, o qual possui as seguintes hipóteses,

$$
    \begin{cases}
H_{0}: \textrm{A homocedasticidade está presente.}\\
H_{1}: \textrm{A homocedasticidade não está presente.}
    \end{cases}
$$

```{r}
library(lmtest)

gqtest(fit1)
```
Usando $\alpha = 0.05$, rejeitamos $H_0$, ou seja, a homocedasticidade não está presente. Assim, o modelo não segue os pressupostos.

<br><br>

Testando múltiplos modelos, usando a variável resposta trasformada e sem trasformar via Box-Cox. O modelo que seguiu os pressupostos foi $\text{mpg} \sim \text{disp + hp + wt + qsec + am}$ usando o a variável transformada via $Y \leftarrow \log(Y)$.

<br><br>

Vendo e testando os pressupostos para tal modelo,

```{r}
summary(fit2)
```
Apenas o intercepto o *wt* foram sugnificativos a um nível de $\alpha = 0.05$, o $R^2$ foi de 0.8842 e o ajustado de 0.8619, a estatísitca F foi significativa.

<br><br>

#### **Testando os pressupostos**

<br><br>
**Não multicolinearidade**

Para isso, será usado Fator de Influência  de Variância (VIF), que é calculado por

$$VIF_j = \frac{1}{1-R^2_j}$$
E é usado como valor crítico o 10, caso o VIF seja maior ou igual há multicolinearidade.

```{r, echo=T}
vif(fit2)
```
Assim o vif das variáveis foi menor que 10, não havendo multicolinearidade, o do disp foi bem alto quase chegando a 10, sendo interessante analisar a variável, como a variável ajuda a cumprir os outros pressupostos será mantida. <br><br>


**Normalidade dos resíduos** <br>

Para isso, será usado o teste de Shapiro-Wilk, que possui as seguintes hipóteses,

$$
    \begin{cases}
H_{0}: \textrm{Os dados seguem uma distribuição normal.}\\
H_{1}: \textrm{Os dados não se distribuem como uma normal.}
    \end{cases}
$$
<br>
```{r}
shapiro.test(fit2$residuals)
```
Assim, utilizando 95% de confiança, não rejeitamos $H_0$, temos que os resíduos seguem uma distribuição normal.
<br><br>

**Homocedastidade**

Para isso, o teste de Goldfeld-Quandt será usado, o qual possui as seguintes hipóteses,

$$
    \begin{cases}
H_{0}: \textrm{A homocedasticidade está presente.}\\
H_{1}: \textrm{A homocedasticidade não está presente.}
    \end{cases}
$$

```{r}
library(lmtest)

gqtest(fit2)
```
Usando 95% de confiança não rejeitamos $H_0$, ou seja, a homocedasticidade está presente nas variáveis independentes.
<br><br>

**Resíduos independentes e não correlacionados**<br>
<br>


Para testar se existe de fato não correlação será usado o teste de Durbin-Watson, que possui as seguintes hipóteses,

$$
    \begin{cases}
H_{0}: \rho = 0,    &&\textrm{Existe independência}\\
H_{1}: \rho \not= 0,&&\textrm{Não existe independência}
    \end{cases}
$$

```{r}
library(car)
durbinWatsonTest(fit2)
```
<br>
Assim, usando 95% de confiança, não rejeitamos $H_0$, ou seja, existe independência nos resíduos.
<br><br>

Assim, todos pressupostos foram aceitos o modelo sendo válido. Enfim, temos o modelo $\text{log(mpg)} \sim \text{disp + hp + wt + qsec + am}$.

<br><br>


9. Utilize o banco de dados bailarinas disponível na pasta Banco de Dados da disciplina e ajuste um MRLM (análise completa) considerando como resposta a variável ped.

Os primeiros dados do banco são,
```{r}
dados <- read.table('dados\\Bailarinas.txt', header = T)
head(dados)
```
<br>
Vizualizando a matriz de correlação,

```{r}
library(corrplot)

corrplot(cor(dados), method = 'color',
         type = 'lower'  ,
         addCoef.col ='black',tl.col="black",
         number.cex = 0.8, diag=F)
```
Inicialmente vemos correlação entre todas variáveis, sendo um péssimo indício. A variável resposta *ped* tem correlação moderada com peso, forte com idade, altura e pee, e quase perfeita com pem. Para as variáveis explicativas há correlção forte ou moderada entre todas.

<br><br>

Fazendo o modelo completo,

```{r}
fit0 <- lm(ped ~ ., data = dados)
summary(fit0)
```
Explicamos perfeitamente ped, com $R^2=1$ e o ajustado também igual a 1, sendo pee e pem significativos.

```{r}

plot(x = dados$ped, y = fit0$fitted.values,
     xlab = 'Ped do banco orignial',
     ylab = 'Ped estimado pelo modelo',
     main = 'Previsto X Real')
curve(x*1,  col = 'red', lwd = 1, add=T, lty=2)
```
O modelo é perfeito devido *PED* ser um função de *PEM* e *PEE*, sendo $PED = 2\cdot PEM - PEE$. Assim não faz sentido fazer uma regressão caso tenhamos essas duas variáveis, pois podemos calcular diretamente e sem erro a variável resposta, por isso, será retirada uma delas do banco original, a variável *PEM*.


<br><br>

```{r}
dados <- dados[, -6]
```

Realizando o novo modelo completo $PED \sim \textrm{PEE} + \textrm{peso} + \textrm{altura} + \textrm{idade}$.

```{r}
fit1 <- lm(ped ~ ., data = dados)
summary(fit1)
```
<br>
Usando o *stepAIC* para encontrar um modelo melhor,

```{r}
library(MASS)
fit <- stepAIC(fit1, direction = 'backward')
```

Assim, o melhor modelo baseado no novo banco e no AIC foi $PED \sim \text{idade} + \text{pee}$, com $AIC=624.13$.

```{r}
summary(fit)
```
Todas as variáveis foram significativas usando $\alpha = 0.05$, o $R^2=0.7125$ e o ajustado 0.7089, tendo um ajuste aceitável, a estatística F também foi significativa.

<br><br>

Verificando os pressupostos,

**Normalidade dos resíduos** <br>

Para isso, será usado o teste de Shapiro-Wilk, que possui as seguintes hipóteses,

$$
    \begin{cases}
H_{0}: \textrm{Os dados seguem uma distribuição normal.}\\
H_{1}: \textrm{Os dados não se distribuem como uma normal.}
    \end{cases}
$$
<br>
```{r}
shapiro.test(fit$residuals)
```
Assim, utilizando 95% de confiança, não rejeitamos $H_0$, temos que os resíduos não seguem uma distribuição normal.
<br><br>
 
E portanto, o modelo não seguirá os pressupostos.

<br><br>

Realizando a transformação Box-Cox buscando atender os pressupostos, a transformação sendo,
$$
    \begin{cases}
Y \rightarrow \frac{Y^\lambda - 1}{\lambda}&,    & \lambda \not = 0;\\
\\
Y \rightarrow \ln(Y)&,& \lambda = 0.
    \end{cases}
$$

<br><br>
O $\lambda$ que foi calculado pelo método e seu intervalo para nossos dados foi 1.7575 e com intervalo,
```{r}
library(MASS)

w = boxcox(ped ~ idade + pee, data = dados, plotit=T)

posicao = which.max(w$y)

lambda = w$x[posicao]
dados$y2 = (dados$ped^(lambda)-1)/lambda
```

Ajustando o modelo com a variável transformada,

```{r}
fit2 <- lm(y2 ~ idade + pee, data = dados)
summary(fit2)
```
Todas as variáveis foram significativas usando $\alpha = 0.01$, o $R^2=0.7264$ e o ajustado 0.723 aumentaram em 0.02, ou seja, praticamente não mudaram, tendo um ajuste aceitável, a estatística F também foi significativa.


<br><br>

#### **Testando os pressupostos**

<br><br>
**Não multicolinearidade**

Para isso, será usado Fator de Influência  de Variância (VIF), que é calculado por

$$VIF_j = \frac{1}{1-R^2_j}$$
E é usado como valor crítico o 10, caso o VIF seja maior ou igual há multicolinearidade.

```{r, echo=T}
vif(fit2)
```
Assim o vif das variáveis foi menor que 10, não havendo multicolinearidade. <br><br>


**Normalidade dos resíduos** <br>

Para isso, será usado o teste de Shapiro-Wilk, que possui as seguintes hipóteses,

$$
    \begin{cases}
H_{0}: \textrm{Os dados seguem uma distribuição normal.}\\
H_{1}: \textrm{Os dados não se distribuem como uma normal.}
    \end{cases}
$$
<br>
```{r}
shapiro.test(fit2$residuals)
```
Assim, utilizando 95% de confiança, não rejeitamos $H_0$, temos que os resíduos seguem uma distribuição normal.
<br><br>

**Homocedastidade**

Para isso, o teste de Goldfeld-Quandt será usado, o qual possui as seguintes hipóteses,

$$
    \begin{cases}
H_{0}: \textrm{A homocedasticidade está presente.}\\
H_{1}: \textrm{A homocedasticidade não está presente.}
    \end{cases}
$$

```{r}
library(lmtest)

gqtest(fit2)
```
Usando 95% de confiança não rejeitamos $H_0$, ou seja, a homocedasticidade está presente nas variáveis independentes.
<br><br>

**Resíduos independentes e não correlacionados**<br>
<br>


Para testar se existe de fato não correlação será usado o teste de Durbin-Watson, que possui as seguintes hipóteses,

$$
    \begin{cases}
H_{0}: \rho = 0,    &&\textrm{Existe independência}\\
H_{1}: \rho \not= 0,&&\textrm{Não existe independência}
    \end{cases}
$$

```{r}
library(car)
durbinWatsonTest(fit2)
```
<br>
Assim, usando 95% de confiança, não rejeitamos $H_0$, ou seja, existe independência nos resíduos.
<br><br>

Assim, todos pressupostos foram aceitos o modelo sendo válido. Enfim, temos o modelo $\text{ped} \sim \text{idade} + \text{pee}$ com ped transformado.