---
output: html_document
title: ' '
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = FALSE)
```

<style>
  body{
    text-align: justify;
    }

</style>

<h3 style="text-align: center;">UNIVERSIDADE ESTADUAL DE MARINGÁ <br> CURSO DE CIÊNCIAS EXATAS <br> CURSO DE ESTATÍSTICA</h3>


<br><br><br><br><br><br><br><br>

<h3 style="text-align: center;">Lorival Coelho Junior</h3>

<br><br><br><br><br><br><br><br>

<h3 style="text-align: center;">Análise de Regressão - Lista 1</h3>

<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>

<h3 style="text-align: center;">Maringá <br> 2023</h3>
<div style="page-break-after: always;"></div>


## Lista de Exercícios I 

1. A reta de regressão obtida pelo método dos mínimos quadrados (MMQ) possui propriedades. Algumas delas necessitam ser demonstradas:
<br>

(a) Que 
$$\sum^n_{i=1}(x_i - \bar x)^2 = \sum^n_{i=1} x_i^2 - n \bar x^2;$$

Abrindo o binômio temos,

$$\sum^n_{i=1}(x_i - \bar x)^2 = \sum^n_{i=1}(x_i^2 - 2x_i\bar x + \bar x^2)$$
Distribuindo o somatório,

$$\sum^n_{i=1}(x_i - \bar x)^2 = \sum^n_{i=1}(x_i^2) - \sum^n_{i=1}(2x_i\bar x) + \sum^n_{i=1}(\bar x^2)$$

$$\sum^n_{i=1}(x_i - \bar x)^2 = \sum^n_{i=1}(x_i^2) - 2 \bar x\sum^n_{i=1}(x_i) + n\bar x^2$$
$$\sum^n_{i=1}(x_i - \bar x)^2 = \sum^n_{i=1}(x_i^2) - 2 \bar x n \bar x + n\bar x^2$$
$$\sum^n_{i=1}(x_i - \bar x)^2 = \sum^n_{i=1}(x_i^2) - 2 n\bar x^2 + n\bar x^2$$

$$\sum^n_{i=1}(x_i - \bar x)^2 = \sum^n_{i=1}(x_i^2) - n\bar x^2$$
<br><br>

(b) A soma dos valores observados é igual à soma dos valores estimados, isto é,

$$\sum^n_{i=1} y_i = \sum^n_{i=1} \hat y_i$$
Sabemos que $\hat y_i = \hat \beta_0 + \hat \beta_1 x_i$ e que  $\hat \beta_0 = \bar y - \hat \beta_1 \bar x$, assim

$$\sum_{i=1}^n \hat y_i = \sum^n_{i=1} \left( \hat \beta_0 + \hat \beta_1 x_i \right) =
\sum_{i=1}^n \left(\bar y - \hat \beta_1 \bar x + \hat \beta_1 x_i\right)$$

$$\sum_{i=1}^n \hat y_i = \sum_{i=1}^n (\bar y) - \sum_{i=1}^n(\hat \beta_1 \bar x) + \sum_{i=1}^n(\hat \beta_1 x_i)$$
$$\sum_{i=1}^n \hat y_i = n\bar y - n\hat \beta_1 \bar x + \hat \beta_1\sum_{i=1}^n( x_i)$$
$$\sum_{i=1}^n \hat y_i = n\bar y - n\hat \beta_1 \bar x + \hat \beta_1 n \bar x$$
$$\sum_{i=1}^n \hat y_i = n\bar y$$
$$\sum^n_{i=1} y_i = n \bar y$$
Portanto,

$$\sum^n_{i=1} y_i = \sum^n_{i=1} \hat y_i =  n \bar y$$
<br><br>

(c) A soma dos resíduos é igual a zero, isto é,

$$\sum^n_{i=1} e_i = 0$$

Sabemos que $e_i = y_i -\hat y_i$, portanto

$$\sum^n_{i=1} e_i = \sum^n_{i=1} (y_i -\hat y_i) = \sum^n_{i=1}y_i -\sum^n_{i=1}\hat y_i $$
$$\sum^n_{i=1} e_i = n \bar y - n \bar y = 0$$
$$\sum^n_{i=1} e_i = 0$$

(d) A soma do produto dos valores estimados e resíduos é igual a zero, isto é,

$$\sum^n_{i=1} \hat y_i e_i = 0$$
Sabemos que $\hat y_i = \beta_0 +\beta_1x_i$,

$$\sum^n_{i=1} \hat y_i e_i =  \sum^n_{i=1}(\beta_0 + \beta_1x_i)e_i$$
$$\sum^n_{i=1} \hat y_i e_i =  \sum^n_{i=1}(\beta_0 e_i) +
\sum^n_{i=1}(\beta_1x_i)e_i$$
$$\sum^n_{i=1} \hat y_i e_i =  \beta_0 \sum^n_{i=1}(e_i) +
\beta_1\sum^n_{i=1}x_ie_i$$

Sabemos que $\sum^n_{i=1} e_i = 0$, assim

$$\sum^n_{i=1} \hat y_i e_i =  \beta_0 \cdot 0 +
\beta_1\sum^n_{i=1}x_i\epsilon_i$$
$$\sum^n_{i=1} \hat y_i e_i = \beta_1\sum^n_{i=1}x_ie_i$$
$$\sum^n_{i=1} \hat y_i e_i = \beta_1\sum^n_{i=1}x_ie_i$$
$$\sum^n_{i=1} \hat y_i e_i = \beta_1\sum^n_{i=1}x_i(y_i - \hat y_i)$$
$$\sum^n_{i=1} \hat y_i e_i = \beta_1\sum^n_{i=1}x_iy_i - \beta_1\sum^n_{i=1}x_i\hat y_i$$
$$\sum^n_{i=1} \hat y_i e_i = \beta_1\left(\sum^n_{i=1}x_i y_i - \sum^n_{i=1}x_i \hat y_i \right)$$

<br><br><br>

2. Faça o que se pede:


(a) Simule n = 30 valores para o Modelo de Regressão Linear Simples (MRLS) (semente 350) dado por: $Y = −2 + 0.5x + \epsilon$ tal que 
  
&emsp;&emsp;$X$∼$N(0, 1)$ e $\epsilon$ ∼ $N(0, \sigma^2 = 9)$;
  
```{r}
set.seed(350)
n    <- 30
x    <- rnorm(n)
erro <- rnorm(n, 0, 3)
y    <- -2 + 0.5*x + erro
```
<br><br>
Os dados simulados por $Y = −2 + 0.5x + \epsilon$, dado que $X$∼$N(0, 1)$ e $\epsilon$ ∼ $N(0, \sigma^2 = 9)$ são os seguintes

```{r}
print(y)
```
<br><br>

<div style="page-break-after: always;"></div>

(b) Apresente o diagrama de dispersão dos dados;

```{r}
plot(x, y,
     xlab = 'X',
     ylab = 'Y',
     main = 'Gráfico de dispersão dos dados simulados'
     )

```
<br>
Os dados se concentram em torno do -2, devido terem sido gerados para isso, pois $Y = −2 + 0.5x + \epsilon$, e $x$ possui média zero e distribuição normal, no entanto mesmo a função $Y$ sendo de uma reta, os dados não distribuem tão bem na forma de uma reta, possivelmente, devido a variância dos resíduos ser relativamente alta $\sigma^2 = 9$.
<br><br>
  
  
(c) Estime os parâmetros do modelo via MMQ e pela forma matricial, isto é,
  $\hat \beta = (X′X)^{-1} X′y$;
  
Via MMQ, temos que $\hat \beta_0 = \bar y - \hat \beta_1 \bar x$ &emsp; e &emsp; $\hat \beta_1 = \frac{\sum^n_{i=1}(x_i - \bar x)(y_i - \bar y)} {\sum^n_{i=1}(x_i - \bar x)^2}$
<br>
Assim, para calcular $\bar \beta_1$, temos $\bar x = 0.04636292$ e $\bar y = -2.176858$, assim, conseguimos um $\beta_1 = 0.26286$ e usando esse para calcular $\beta_0$ temos que esse vale $-2.189045$. <br> <br>

<div style="page-break-after: always;"></div>

Assim teríamos que a reta de regressão é dada por $-2.189045 + 0.26286x$, plotando-a nos dados teríamos o seguinte 

```{r}
beta_1 = sum((x - mean(x)) * (y - mean(y))) / sum((x - mean(x))^2)

beta_0 = mean(y) - beta_1*mean(x)

plot(x, y,
     xlab = 'X',
     ylab = 'Y',
     main = 'Gráfico de dispersão dos dados simulados'
     )
curve(beta_0 + beta_1*x, add =T, col = 'red', lwd = 2)

```

<br>

<div style="page-break-after: always;"></div>

Pela forma matricial precisamos calular $\hat \beta = (X′X)^{-1} X′Y$, para isso, primeiramente precisamos definir as matrizes $X$ e $Y$, temos que $X$ é
```{r}
matriz_x = matrix(c(rep(1,n),x), ncol =2)
matriz_x
```

<div style="page-break-after: always;"></div>

E que $Y$ é,

```{r}
matriz_y = matrix(y)
matriz_y
```

Precisamos também encontrar a transposta de $X$, a matriz $X'$, sendo essa

```{r}
t(matriz_x)
```

<div style="page-break-after: always;"></div>

calculando $X'X$, temos

```{r}
t(matriz_x) %*% matriz_x
```

Agora invertendo essa matriz, temos

```{r}
solve(t(matriz_x) %*% matriz_x)
```

Calculando $X' Y$,


```{r}
t(matriz_x) %*% matriz_y
```

Agora calculando $\hat \beta$ de fato,

```{r}
solve(t(matriz_x) %*% matriz_x) %*% t(matriz_x) %*% matriz_y
```
Assim temos que $\beta_0 = -2.189045$ e $\beta_1 = 0.262860$, e a reta de regressão  $-2.189045 + 0.262860x$, o mesmo valor calculado de forma analítica. Plotando essa função teríamos a seguinte reta, <br>

```{r}
betas <- solve(t(matriz_x) %*% matriz_x) %*% t(matriz_x) %*% matriz_y

plot(x, y,
     xlab = 'X',
     ylab = 'Y',
     main = 'Gráfico de dispersão dos dados simulados'
     )
curve(betas[1] + betas[2]*x, add =T, col = 'red', lwd = 2)
```
<br><br>

(d) Apresente o histograma dos erros e teste sua normalidade via Shapiro-Wilk.
 
```{r}
hist(erro, 
     xlab = 'Erro',
     ylab = 'Frequência',
     main = 'Histograma dos erros')

```
<br>
Podemos vizualizar que os valores mais frequentes se encontram de -2 a 2, com caudas indo de -6 a -2 e de 2 a 8.
<br>
Agora para testar a normalidade dos resíduos será usado o teste de Shapiro-Wilk que possui as seguintes hipóteses

$$
    \begin{cases}
H_{0}: \textrm{Os dados seguem uma distribuição normal.}\\
H_{1}: \textrm{Os dados não se distribuem como uma normal.}
    \end{cases}
$$

```{r}
shapiro.test(erro)
```
Verificando a normalidade dos resíduos, temos que utilizando 95% de confiança, não rejeitamos $H_0$, ou seja, temos indícios de que os erros seguem normalidade.

<br><br> 
<div style="page-break-after: always;"></div>

3) Com o objetivo de se estudar a relação entre Tempo de uma reação química (resposta) e Temperatura, um certo experimento foi realizado. A Tabela 1 a seguir contém os valores das temperaturas, em $°C$, e os tempos obtidos, em segundos

```{r}
L1 <- c(11.3, 11.8, 11.5, 12.1, 11.7)
L2 <- c(11.8, 11.5, 11.4, 11.7, 11.2)
L3 <- c(10.9, 11.2, 10.8, 10.6, 10.3)
L4 <- c(10.4, 9.8 , 9.5 , 9.9 , 9.2 )
L5 <- c(9.6 , 9.0 , 8.7 , 8.3 , 9.1 )
L6 <- c(9.1 , 9.3 , 8.5 , 8.6 , 8.3 )
L7 <- c(8.4 , 8.1 , 8.1 , 7.7 , 7.9 )

temperatura <- c(rep(20, 5), rep(30, 5), rep(40, 5),
                 rep(50, 5), rep(60, 5), rep(70, 5),
                 rep(80, 5))

tempo       <- c(L1, L2, L3, L4, L5, L6, L7)


dados       <- data.frame(Temperatura <- temperatura,
                          Tempo       <- tempo)

colnames(dados) <- c('Temperatura', 'Tempo')

print(dados)

```
<br><br>

<div style="page-break-after: always;"></div>

a) Apresente o diagrama de dispersão dos dados;

```{r}
plot(dados$Temperatura, dados$Tempo,
     xlab = 'Temperatura',
     ylab = 'Tempo',
     main = 'Gráfico de dispersão do tempo pela temperatura')
```
<br>
O gráfico de dispersão é no eixo tempo contínuo e no eixo temperatura discreto, os pontos formam, aparentemente, uma reta.

<br><br>

(b) Estime a média, desvio-padrão, variância e coeficiente de variação para a resposta (Tempo de reação), considerando-a como normalmente distribuiída (Sugestão: Utilize a função fitdistr da livraria MASS do R)

```{r}
library(MASS)

est_L1 <- fitdistr(L1, dnorm, start = list(mean= 10, sd= 2))$estimate
est_L2 <- fitdistr(L2, dnorm, start = list(mean= 10, sd= 2))$estimate
est_L3 <- fitdistr(L3, dnorm, start = list(mean= 10, sd= 2))$estimate
est_L4 <- fitdistr(L4, dnorm, start = list(mean= 10, sd= 2))$estimate
est_L5 <- fitdistr(L5, dnorm, start = list(mean= 10, sd= 2))$estimate
est_L6 <- fitdistr(L6, dnorm, start = list(mean= 10, sd= 2))$estimate
est_L7 <- fitdistr(L7, dnorm, start = list(mean= 10, sd= 2))$estimate

estimativas <- data.frame(rbind(est_L1, est_L2, est_L3,
                     est_L4, est_L5, est_L6, est_L7))

estimativas$var <- estimativas$sd**2
estimativas$cv  <- estimativas$sd / estimativas$mean

row.names(estimativas) <- c(seq(20, 80, 10))

print(estimativas)

```

Estimando as estatísticas via o *fitdistr*, temos um coeficiente de variação baixo para todas temperaturas, ou seja, a média pode ser usada para representar os dados, e temos o comportamento esperado de acordo com o gráfico para as médias, conforme a temperatura diminui a média do tempo aumenta, quanto a dispersão ela é maior nas temperaturas de 50 a 70 graus.

<br><br>

<div style="page-break-after: always;"></div>

(c) Calcule a média, desvio-padrão, variância e coeficiente de variação para a resposta em cada nível de temperatura e responda se há algum indício de heterocedasticidade. Justifique;

```{r}
tempos      <- data.frame(T20 = L1, T30 = L2, T40 = L3, T50 = L4,
                          T60 = L5, T70 = L6, T80 = L7 )


estimativas <- data.frame(
                          apply(tempos, 2, FUN = function(x)
                                  {media  <- mean(x)
                                   desvio <- sd(x)
                                   varia  <- var(x)
                                   cv     <- sd(x)/mean(x)
                                   return(c(media, desvio, varia, cv))
                                   }))

row.names(estimativas) <- c('mean', 'sd', 'var', 'cv')
estimativas            <- data.frame(t(estimativas))

estimativas

```
Sem realizar testes, eu diria que, aparentemente, não há heterocedasticidade, pois, a variância tem um aumento na temperatura 50, 60 e 70, mas depois disso ela abaixa novamente, não explodindo conforme a variável x aumenta, como é comum no caso de heterocedasticidade. No entanto, para verificar se a variação da variância pode ser considerada alta o suficiente e por conseguinte não existir homocedasticidade, é necessário fazer os testes específicos para isso.

<br><br>

(d) Estime os parâmetros do MRLS da Temperatura versus Tempo via comando *lm* do R;

```{r}

modelo1 <- lm(Tempo ~ Temperatura, data = dados)
modelo1
```
Assim temos o seguinte modelo, *Tempo* = 13.18357 - 0.06521 $\cdot$ *Temperatura*.

E agora que temos um modelo, podemos verificar a heterocedasticidade do exercício anterior. Para isso, o teste de Goldfeld-Quandt será usado, o qual possui as seguintes hipóteses,

$$
    \begin{cases}
H_{0}: \textrm{A homocedasticidade está presente.}\\
H_{1}: \textrm{A homocedasticidade não está presente.}
    \end{cases}
$$
```{r}

library(lmtest)

gqtest(modelo1)

```
Assim, considerando uma confiança de 95%, não rejeitamos $H_0$, ou seja, a homocedasticidade está presente.

<br><br>

(e) Teste a significância dos parâmetros em nível de 5%;

Para testar a significância de $\beta_1$, temos dois passos a serem seguidos,

- Usar um teste t para as seguintes hipóteses
$$
    \begin{cases}
H_{0}: {\beta_1 = 0}\\
H_{1}: {\beta_1 \not = 0}
    \end{cases}
$$
- Usar o teste F ao ajustamento global do modelo.
<br>
Realizando o teste t,

$$t_c = \frac{\hat \beta_1 - \beta_1}{\sqrt{\frac{QME}{Sxx}}}$$
$t_c$ &nbsp;~&nbsp; $t_{ n-2, \alpha}$ <br>

$$S_{xx} = \sum^n_{i=1} (x_i - \bar x)^2 = 14000$$
$$QME = \frac{SQE}{(n-2)} =  \frac{\sum^n_{i=1}(y_i - \hat y_i)^2}{(n-2)}$$
$$QME = \frac{5.381071}{35}= 0.1537449$$
$$t_c = \frac{\hat \beta_1 - \beta_1}{\sqrt{\frac{QME}{Sxx}}}$$
$$t_c = \frac{-0.06521429 - 0}{\sqrt{\frac{0.1537449}{14000}}} = -19.67915$$
Temos que a área de não rejeição dada por $t_{33, 0.025} < t_c < t_{33, 0.975}$, pegando os valores de $t$ tabelados, temos que essa área é dada por $-2.034515 < t_c < 2.034515$. Portanto $t_c < -2.034515$, e então com 95% de confiança rejeitamos $H_0$, ou seja, $\beta_1$ é diferente de zero.
```{r}
residuo <- Tempo - modelo1$fitted.values
tc <- (modelo1$coefficients[2] - 0)/((sum(residuo^2))/length(residuo)/ sum((Temperatura - mean(Temperatura))^2))^0.5

#qt(0.975, 33)
```
<br><br>

(f) Teste a normalidade nos resíduos;

Para testar a normalidade dos resíduos será usado o teste de Shapiro-Wilk, que possui as seguintes hipóteses,

$$
    \begin{cases}
H_{0}: \textrm{Os dados seguem uma distribuição normal.}\\
H_{1}: \textrm{Os dados não se distribuem como uma normal.}
    \end{cases}
$$

```{r}
shapiro.test(modelo1$residuals)
```
Assim, com 95% de confiança não rejeitamos $H_0$, ou seja, os resíduos do modelo seguem a distribuição normal, assim esse pressuposto está sendo seguido.

<div style="page-break-after: always;"></div>

(g) Trace a reta estimada sobre os pontos observados;

```{r}
plot(dados$Temperatura, dados$Tempo,
     xlab = 'Temperatura',
     ylab = 'Tempo',
     main = 'Gráfico de dispersão do tempo pela temperatura')
curve(modelo1$coefficients[1] + modelo1$coefficients[2]*x, add = T, col = 'red', lty = 1, lwd = 2)

```
<br>
Assim como visualizado no gráfico de dispersão inicial, os pontos formam uma espécie de reta, e o modelo consegue ajustar uma reta de regressão que passe pelo meio de todos pontos nos grupos de temperatura. <br><br>

(h) Construa intervalos de 95% de confiança para os parâmetros estimados;

```{r}
confint(modelo1)
```
Temos que com 95% de confiança, $\beta_0$ está entre 12.80965603 e 13.55748683, enquanto $\beta_1$ está entre -0.07215772 e  -0.05827085.
<br><br>

<div style="page-break-after: always;"></div>

4. As tabelas 2 e 3 a seguir, fazem parte da saída de uma análise de regressão realizada em um determinado programa estatístico.
<br><br>

\begin{array}{ccccc}
\hline
\textrm{Parameter}&\textrm{Estimate}&\textrm{Std. Error}&\textrm{t value}&\textrm{p}\\
\hline

                \textrm{Intercept}&83.07408&6.5930&12.60&0.000\\
                \textrm{X}&-1.1848&0.1258&-9.42&0.000\\

\hline
\end{array}

<br><br>

\begin{array}{ccccc}
\hline
\textrm{FV}&\textrm{Df}&\textrm{Sum Sq}&\textrm{Mean Sq}&\textrm{F value}&\textrm{p}\\
\hline

                \textrm{X}&1&1021.1&1021.1&88.68&0.000\\
                \textrm{Residuals}&7&80.6&11.5\\
                \textrm{Total}&8&1101.6\\

\hline
\end{array}




(a) Escreva a equação da reta ajustada;

A equação da reta é dada por $y = 83.074 -1.1848x$, possuindo o seguinte gráfico,

```{r}
curve( 83.074 -1.1848*x, 0, 5, lwd =2, col = 'red')
```
<br><br>

(b) Encontre um intervalo de confiança de 95%, para os coeficientes da reta ajusta;

```{r}



IC <- matrix(c(83.074 -qnorm(0.975)*6.5930, 83.074 +qnorm(0.975)*6.5930,
         -1.1848-qnorm(0.975)*0.1258, -1.1848+qnorm(0.975)*0.1258),
       nrow = 2, byrow = T,dimnames =  list('  ' = c('Intercept', 'x'), ' ' = c('2.5%', '97.5%')) )
IC
```

<br><br>


(c) Verifique se o modelo contribui para explicar a variável resposta. Está adequado? Justifique;

Temos o $p$ é inferior a 0.001 no teste t para ambas variáveis, assim elas são significativas para explicar a variável resposta, assim como o $p$ da estatística F indíca, sendo também menor que 0.001
<br><br>


(d) Encontre a estimativa da variância residual;


Pegando da tabela Anova temos que $SQE = 80,6$. <br>

Sabendo que a variância dos resíduos pode ser calculada por $\frac{SQE}{Df}$,

$$Var(e_i) = \frac{SQE}{Df} = \frac{80.6}{7} =  11.51429$$



<br><br>

5. Um experimento foi conduzido para avaliar, em coelhos a disponibilidade relativa (DR) do Fósforo existente nos Fosfatos da rocha Araxá e de Patos de Minas em relação ao Fósforo existente no Fostato Bicálcico. Os animais foram alimentados com rações contendo níveis crescentes de cada Fosfato  e foram anotados os consumos de ração, o que permitiu calcular o consumo de Fósforo em cada unidade experimental. A variável resposta observada foi a resistência do fêmur à quebra, mensurada com dinamômetro (Tabela 4).

A disponibilidade relativa é estimada pela razão entre o coeficiente linear de regressão (angular no caso da reta) obtido para o Fosfato de interesse e para o Fosfato Bicálcico, isto é:

$$DR^* = \beta_1^* / \beta_1^{Bicalcico}$$
<br>

\begin{array}{cc|cc|cc}
\hline
\textrm{Araxa}  &&\textrm{Patos}&&\textrm{Bicalcico}&\\
\hline
\textrm{Y}      &\textrm{X}&\textrm{Y}&\textrm{X}&\textrm{Y}&\textrm{X}\\
\hline
                21.28&0.184&23.50&0.195&25&0.18\\
                31.60&0.350&34.59&0.350&39&0.36\\
                32.42&0.516&31.18&0.505&43&0.50\\
                41.74&0.683&43.27&0.661&60&0.65\\
                420.6&0.849&40.36&0.816&63&0.78\\
                53.38&1.015&50.45&0.972&71&0.94\\
\hline
\end{array}

(a) Ajuste os modelos individuais;

```{r}
araxa <- data.frame(Y = c(21.28,31.60,32.42,41.74,42.06,53.38),
                    X = c(0.184,0.350,0.516,0.683,0.849,1.015))
patos <- data.frame(Y = c(23.50,34.59,31.18,43.27,40.36,50.45),
                    X = c(0.195,0.350,0.505,0.661,0.816,0.972))
bical <- data.frame(Y = c(25,39,43,60,63,71),
                    X = c(0.18,0.36,0.50,0.65,0.78,0.94))

modelo_araxa <- lm(Y ~ X, data = araxa)
modelo_patos <- lm(Y ~ X, data = patos)
modelo_bical <- lm(Y ~ X, data = bical)
```
<br>

**Modelo Araxá**: $Y = 16.35 + 34.58 \cdot X$

O ajuste aos dados é o seguinte,

```{r}
plot(x = araxa$X, y = araxa$Y,
     xlab = 'Fósforo',
     ylab = 'Resistência do Fêmur à quebra',
     main = 'Gráfico de dispersão e ajuste do modelo araxá ')
curve(modelo_araxa$coefficients[1] + modelo_araxa$coefficients[2]*x ,  col = 'red', lwd = 2, add=T)


```
Aparentemente, o modelo consegue explicar os pontos, esses estando bem próximos a reta. <br>

**Modelo Patos**: $Y = 19.62 + 30.19 \cdot X$

O ajuste aos dados é o seguinte,

```{r}
plot(x = patos$X, y = patos$Y,
     xlab = 'Fósforo',
     ylab = 'Resistência do Fêmur à quebra',
     main = 'Gráfico de dispersão e ajuste do modelo Patos ')
curve(modelo_patos$coefficients[1] + modelo_patos$coefficients[2]*x ,  col = 'red', lwd = 2, add=T)


```
Aparentemente, o modelo consegue explicar os pontos, esses estando em torno da reta. 
<br>
**Modelo Bicálcico**: $Y = 15.27 + 61.40 \cdot X$

O ajuste aos dados é o seguinte,

```{r}
plot(x = bical$X, y = bical$Y,
     xlab = 'Fósforo',
     ylab = 'Resistência do Fêmur à quebra',
     main = 'Gráfico de dispersão e ajuste do modelo Bicálcico ')
curve(modelo_bical$coefficients[1] + modelo_bical$coefficients[2]*x ,  col = 'red', lwd = 2, add=T)


```
Aparentemente, o modelo consegue explicar os pontos, esses estando bem próximos a reta.

<br>

Assim os 3 modelos conseguiram se ajustar, aparentemente, bem aos dados, visto que esses se distribuim em forma de reta.

<br>

```{r}
summary(modelo_araxa)
summary(modelo_patos)
summary(modelo_bical)
```

<br>

Para todos modelos, temos que tanto o intercepto quanto o X é significativo usando 99% de confiança, com um $R^2$ maior que 0.8 chegando a 0.96.

<br><br>

(b) Calcule a disponibilidade relativa do Fósforo nos dois Fosfatos de rocha e decida pelo melhor;

Temos que

$$DR_{araxa} =  \beta_1^{araxa} / \beta_1^{Bicalcico} = \frac{34.57651}{61.39689} = 0.5631639$$
$$DR_{patos} =  \beta_1^{patos} / \beta_1^{Bicalcico} = \frac{30.18677}{61.39689} = 0.4916661$$
Assim, a rocha de Araxá tem um $DR$ maior que a de Patos de Minas, e por consequência possui mais Fósforo, sendo melhor.


```{r}
DR_araxa <- modelo_araxa$coefficients[2] / modelo_bical$coefficients[2]
 # Quanto mais melhor
DR_patos <- modelo_patos$coefficients[2] / modelo_bical$coefficients[2]
```
<br><br>


(c) Trace as retas ajustadas aos dados simultaneamente.

```{r}
dados <- rbind(araxa, patos, bical)

plot(x = dados$X, y = dados$Y,
     xlab = 'Fósforo',
     ylab = 'Resistência do Fêmur à quebra',
     main = 'Gráfico de dispersão e ajuste do modelos')
curve(modelo_araxa$coefficients[1] + modelo_araxa$coefficients[2]*x ,  col = 'red', lwd = 2, add=T, lty=2)
curve(modelo_patos$coefficients[1] + modelo_patos$coefficients[2]*x ,  col = 'blue', lwd = 2, add=T, lty=2)
curve(modelo_bical$coefficients[1] + modelo_bical$coefficients[2]*x ,  col = 'green3', lwd = 2, add=T, lty=2)

text(x=0.8, y=70, "Bicálcico",col = 'green3')
text(x=1, y=46, "Patos",col = 'blue')
text(x=0.9, y=53, "Araxá",col = 'red')
```


<br><br>

<div style="page-break-after: always;"></div>

6. Com base nos dados do Instituto Nacional de Estatística de Portugal (INE), o arquivo **cereais** contém a evolução da superfície agrícola (y) utilizada anualmente na produção de cereais para grão (y: **area**, em $km^2$) em Portugal, no éríodo de 1968 a 2011 (x: **ano**). Faça o que se pede:
```{r}
cereais <- read.table(file = 'dados/cereais.txt',header = T)
cereais
```

<div style="page-break-after: always;"></div>

(a) Construa uma nuvem de pontos de superfície agrícola vs. ano e comente;

```{r}

plot(x = cereais$ano, y = cereais$area,
     xlab = 'Ano',
     ylab = 'Superfície agrícola',
     main = 'Gráfico de dispersão do ano pela área agrícola')
```
<br>
No gráfico acima, podemos verificar que os pontos de superfície e ano formam uma reta, uma aparente correlação linear negativa.
<br><br>

(b) A partir do gráfico obtido do item anterior, sugira um valor para o coeficiente de correlação entre superfície agrícola e ano. Depois, utilize os comandos do R para calcular esse mesmo coeficiente de correlação. Comente o seu significado;
<br>
Apenas pelo gráfico de dispersão, a correlação parece ser bem alta pelos dados formarem uma reta muito bem definida e com os pontos próximo, sugerindo seria um valor de $\rho=-0.9$.

<br>

Para calcular a correlação será usado o coeficiente de correlação de Pearson, visto que esse é usado para dados que tem relação linear, o coeficiente possui a seguinte fórmula,

$$\rho = \frac{cov(X,Y)}{\sqrt{var(X) \cdot var(Y)}}$$

```{r, echo = T}
cor(cereais$ano, cereais$area)
```
<br>
O $\rho = -0.9826927$, representa uma correlação inversa quase perfeita, como era observado no gráfico que forma uma reta, demonstrando que, possivelmente, o ano tem grande influência na superfície agrícola.
<br><br>

<div style="page-break-after: always;"></div>

(c) Ajuste uma reta de regressão de superfície agrícola utilizada sobre anos. Discuta o significado dos parâmetros da reta ajustada, no contexto do problema sob estudo;

<br>

O modelo linear possui a seguinte fórmula $Y = 523001.7 -258.8X$.

```{r}
modelo1 <- lm(area ~ ano, data = cereais)

plot(x = cereais$ano, y = cereais$area,
     xlab = 'Ano',
     ylab = 'Superfície agrícola',
     main = 'Gráfico de dispersão do ano pela área agrícola')
curve(modelo1$coefficients[1] + modelo1$coefficients[2]*x, add=T,
      lwd = 2, col = 'red')
summary(modelo1)
```
<br>
Ambos parâmetros são significativos a 99.99%, temos que o Intercepto = 523001.7, ele é bem alto devido o ano estar na casa de $10^3$ diminuindo bastante esse valor, quanto ao ano = -258.8, ele é negativo assim diminuindo o valor do intercepto conforme o ano aumenta. 

Pensando no significado puro, o intercepto ou $\beta_0$ seria o valor em que a reta corta o eixo $y$ em $x=0$, enquanto o *ano* ou $\beta_1$ é o coeficiente angular da reta, o parâmetro que define o ângulo da reta.
<br><br>

(d) Comente a qualidade da reta obtida, calculando o respectivo coeficiente de determinação e interpretando o valor obtido;
<br>
Temos que o coeficiente de determinação $(R^2)$ pode ser usado para testar a qualidade do ajuste, sendo calculado por

$$R^2 = \frac{SQReg}{SQT} = 
\frac{\sum^n_{i=1}(\hat Y_i - \bar Y)^2}{\sum^n_{i=1} (Y_i - \bar Y)^2}$$
<br>

$$R^2 = \frac{97924480}{101404176} =  0.965684$$
```{r, echo = F }
R2 <- sum((modelo1$fitted.values - mean(cereais$area))^2)/sum((cereais$area - mean(cereais$area))^2)
#R2
```

No nosso modelo, temos que $R^2 =  0.9656849$, agora calculando o coeficiente de determinação ajustado, usando
<br>
$$\bar R^2 = R^2 - \frac{1}{n-2}(1 - R^2)$$
$$\bar R^2 = 0.9656849^2 = \frac{1}{35-2}(1 - 0.9656849) = 0.9642551$$
```{r, echo = F}
R2 - (1)/(nrow(cereais)-2) * (1 - R2)
```
<br>
Temos que $\bar R^2 = 0.9642551$, lembrando que $0 < R^2 < 1$ e quanto mais perto de 1 o $R$, melhor o ajuste do modelo, assim de acordo apenas com o coeficiente de determinação ajustado o modelo está explicando quase perfeitamente (96,42%) os dados .

<br><br>

<div style="page-break-after: always;"></div>

(e) Trace a reta de regressão ajustada em cima da nuvem de pontos e comente;

```{r}
plot(x = cereais$ano, y = cereais$area,
     xlab = 'Ano',
     ylab = 'Superfície agrícola',
     main = 'Gráfico de dispersão do ano pela área agrícola')
curve(modelo1$coefficients[1] + modelo1$coefficients[2]*x, add=T,
      lwd = 2, col = 'red')
```
<br>

A reta se ajusta bem aos dados, visto esses terem uma alta relação linear, com os pontos bem próximos a reta <br><br>

(f) Calcule a Soma de Quadrados Total (SQT), a partir do cálculo da variância amostral de y;
<br>




$$SQT = \sum^n_{i=1}(Y_i - \bar Y)^2 = 101404176$$
```{r}
#sum((cereais$area - mean(cereais$area))^2)
```
<br><br>

(g) Calcule o valor da Soma de Quadrados de Regressão (SQReg);
<br>

$$SQReg = \sum^n_{i=1}(\hat Y_i - \bar Y)^2 = 97924480 $$
```{r}
#sum((modelo1$fitted.values - mean(cereais$area))^2)
```

<br><br>
<div style="page-break-after: always;"></div>

(h) Calcule a Soma de Quadrados dos Resíduos (SQE), diretamente a partir dos resíduos, e verifique numericamente a relação fundamental da Regressão Linear: $SQT = SQReg + SQE$;
<br>
De exercícios anteriores temos que $SQReg = 97924480$ e $SQT = 101404176$.
<br>

$$SQT = SQReg + SQE$$
$$101404176 = 97924480 + SQE$$
$$101404176 - 97924480 = SQE$$
$$SQE = 3479696$$
<br>
Calculando diretamente pelos resíduos,

```{r, echo = T}
sum(modelo1$residuals^2)
```
<br>
Tem a diferença de 1 unidade que é devido a aproximação. <br><br>

(i) Altere as unidades de medida da variável área, de $km^2$ para hectares (*area* $\rightarrow$ *area* $\cdot$ 100). Ajuste novamente a regressão, após efetuar essta alteração. O que aconteceu aos parâmetros estimados e ao coeficiente de determinação $R^2$? Comente;

```{r}
cereais$hectares <- cereais$area * 100
cereais


modelo2 <- lm(hectares ~ ano, data = cereais)

```
<br>
Ajustando o modelo da área em hectares sendo explicado pelo ano, temos $Y = 52300171 - 25876\cdot X$. 

Comparando com o modelo da área em $km^2$ sendo explicado pelo ano, tínhamos $Y = 523001.71 -258.76 \cdot X$, ou seja, a variável explicativa foi multiplicada por 100 e os coeficientes do modelo também.
<br>

Agora olhando o modelo ajustado aos pontos graficamente,

```{r}
plot(x = cereais$ano, y = cereais$hectares,
     xlab = 'Ano',
     ylab = 'Superfície agrícola',
     main = 'Gráfico de dispersão do ano pela área agrícola em hectares')
curve(modelo2$coefficients[1] + modelo2$coefficients[2]*x, add=T,
      lwd = 2, col = 'red')
```

O gráfico não mudou nada, apenas a escala do eixo y.

```{r}
summary(modelo2)
```
<br>
Temos que o coeficiente de determinação ajustado do modelo por hectare é $\bar R^2 = 0.9642551$, enquanto o do modelo por área em $km^2$ era $\bar R^2 = 0.9642551$. Ou seja, não mudaram.
<br><br>


(j) De novo a partir dos dados originais, transforme a variável ano num contador dos anos do estudo (*ano* $\rightarrow$ *ano* - 1985). Ajuste novamente a regressão, após efetuar esta alteração. O que aconteceu aos parâmetros estimados e ao coeficiente de determinação $R^2$? Comente.
<br>

```{r}
cereais <- cereais[-3]
cereais['data_ajustada'] <- cereais$ano - 1985
cereais

modelo3 <- lm(area ~ data_ajustada, data = cereais)
modelo3
```

Ajustando o modelo da área em hectares sendo explicado pelo ano, temos $Y = 9362.5 - 258.76 \cdot X$. 

Comparando com o modelo inicial que era $Y = 523001.71 -258.76 \cdot X$, temos uma mudança no intercepto, diminuindo esse visto que o segundo parâmetro vai ser multiplicado por um $X$ menor, já para o $\beta_1$ não houve mudança, continuou com o mesmo valor.
<br>

```{r}
plot(x = cereais$data_ajustada, y = cereais$area,
     xlab = 'Ano ajustado',
     ylab = 'Superfície agrícola',
     main = 'Gráfico de dispersão do ano pela área agrícola em hectares')
curve(modelo3$coefficients[1] + modelo3$coefficients[2]*x, add=T,
      lwd = 2, col = 'red')
```

O gráfico, novamente, é idêntico, apenas mudando a escala do eixo x.
<br>
```{r}
summary(modelo3)
```
<br>
O $\bar R^2$ não sofreu mudanças também.

<br><br>
<div style="page-break-after: always;"></div>

7. Hsuie, Ma e Tsai (1995) estudam o efeito da **razão** molar do ácido sebácico (o regressor) na **viscosidade** intrínseca dos copoliesteres (a resposta).
<br>

\begin{array}{c|cccccccc}
\hline
\textrm{Razão}            &1&0.9&0.8&0.7&0.6&0.5&0.4&0.3 \\
\hline
\textrm{Viscosidade}      &0.45&0.20&0.34&0.58&0.70&0.57&0.55&0.75\\
\hline
\end{array}
<br>

(a) Realize uma Análise de RLS completa e apropriada;
<br>

```{r}
dados <- data.frame(razao = seq(1, 0.3, -0.1),
                    viscosidade = c(0.45, 0.2, 0.34, 0.58, 0.7, 0.57, 0.55, 0.75))
  
plot(x = dados$razao, y = dados$viscosidade,
     xlab = 'Razão molar',
     ylab = 'Viscosidade',
     main = 'Gráfico de dispersão da viscosidade pela razão molar')

```
<br>
Apenas olhando o gráfico de dispersão, talvez uma reta não se ajuste, possívelmente, uma função de terceiro ou quarto grau faria um ajuste melhor. Mas será tentado um modelo linear simples inicialmente.<br>

```{r}
modelo1 <- lm(viscosidade ~ razao, data = dados)
summary(modelo1)
```
<br>
O modelo linear simples estimado foi $Y = 0.8781 -0.5548 \cdot X$, com o intercepto sendo significante a $99.99$% de confiança e a razão significante a $95$%, possuindo um coeficiente de determinação ajustada de $\bar R^2 = 0.4835$, ou seja, baseado apenas no R-ajustado um ajuste ruim.
<br><br>

```{r}

plot(x = dados$razao, y = dados$viscosidade,
     xlab = 'Razão molar',
     ylab = 'Viscosidade',
     main = 'Gráfico de dispersão da viscosidade pela razão molar')
curve(modelo1$coefficients[1] + modelo1$coefficients[2]*x, add = T, col = 'red', lty = 1, lwd = 2)
```
<br>
A reta ficou no meio dos pontos, mas não se ajustou bem.
<br><br>
Verificando os pressupostos do modelo,

<div style="page-break-after: always;"></div>

**Normalidade dos resíduos** <br>

Para isso, será usado o teste de Shapiro-Wilk, que possui as seguintes hipóteses,

$$
    \begin{cases}
H_{0}: \textrm{Os dados seguem uma distribuição normal.}\\
H_{1}: \textrm{Os dados não se distribuem como uma normal.}
    \end{cases}
$$
<br>
```{r}
shapiro.test(modelo1$residuals)
```
Assim, utilizando 95% de confiança, não rejeitamos $H_0$, temos que os resíduos seguem uma distribuição normal.
<br><br>

**Homocedastidade**

Para isso, o teste de Goldfeld-Quandt será usado, o qual possui as seguintes hipóteses,

$$
    \begin{cases}
H_{0}: \textrm{A homocedasticidade está presente.}\\
H_{1}: \textrm{A homocedasticidade não está presente.}
    \end{cases}
$$

```{r}
library(lmtest)

gqtest(modelo1)
```
Usando 95% de confiança não rejeitamos $H_0$, ou seja, a homocedasticidade está presente nas variáveis independentes.
<br><br>

**Resíduos independentes e não correlacionados**<br>
<br>
Análise gráfica,
```{r}

```
<br>


Para testar se existe de fato não correlação será usado o teste de Durbin-Watson, que possui as seguintes hipóteses,

$$
    \begin{cases}
H_{0}: \rho = 0,    &&\textrm{Existe independência}\\
H_{1}: \rho \not= 0,&&\textrm{Não existe independência}
    \end{cases}
$$

```{r}
library(car)
durbinWatsonTest(modelo1)
```
<br>
Assim, usando 95% de confiança, não rejeitamos $H_0$, ou seja, existe independência nos resíduos.
<br><br>

Assim, todos os pressupostos do modelo foram atendidos.
<br>
<br>
Para tentar melhorar o modelo será feita a transformação de Box-Cox que é a seguinte, <br>

$$
    \begin{cases}
Y \rightarrow \frac{Y^\lambda - 1}{\lambda}&,    & \lambda \not = 0;\\
\\
Y \rightarrow \ln(Y)&,& \lambda = 0.
    \end{cases}
$$
<br><br>

O $\lambda$ que foi calculado pelo método e seu intervalo para nossos dados foi o seguinte,
```{r}
library(MASS)

w = boxcox(viscosidade ~ razao, data=dados, plotit=T)

posicao = which.max(w$y)

lambda = w$x[posicao]
dados$y2 = (dados$viscosidade^(lambda)-1)/lambda
modelo2 <- lm(y2 ~ razao, data = dados)
```
<br>

Daí temos que o $\lambda = 1.636364$, com o intervalo de confiança não contendo o zero por pouco. Assim, nossa transformação será $Y \rightarrow \frac{Y^{1.6363}- 1}{1.6363}$.

```{r}


plot(x = dados$razao, y = dados$y2,
     xlab = 'Razão molar',
     ylab = 'Viscosidade',
     main = 'Gráfico de dispersão da viscosidade pela razão molar')
curve(modelo2$coefficients[1] + modelo2$coefficients[2]*x, add = T, col = 'red', lty = 1, lwd = 2)

shapiro.test(modelo2$residuals)
gqtest(modelo2)
durbinWatsonTest(modelo2)
```


O Ajuste não parece ter melhorado muito, e já realizado todos os testes para verificar os pressupostos, o modelo passou.
<br>

```{r}

summary(modelo2)

```
Olhando o $\bar R^2$ aumentou de 0.4835 para 0.5075, o intercepto não foi significativo, e a razão foi significativa a 99.99%. 
<br><br>
Agora em mais uma tentativa, o intercepto será tirado do modelo.
<br><br>


<div style="page-break-after: always;"></div>

(b) Apresente um gráfico simultâneo com os dados, o ajuste, o IC e o IP;
<br>
Por não haver uma grande diferença no ajuste dos modelos, será optado pelo primeiro sem a transformação.

```{r}

ICs <- confint(modelo1)

y.pred      = predict(modelo1)
Interv.conf = predict(modelo1,interval="confidence")
Interv.pred = predict(modelo1,interval="prediction")
m           = matrix(cbind(Interv.conf,Interv.pred[,2:3]),ncol=5)
nomes       = list(NULL,c("y.predito","Linf.IC","Lsup.IC","Linf.IP","Lsup.IP"))
dimnames(m) = nomes
R2 <- summary(modelo1)$adj.r.squared
#--- Grafico do ajuste completo ---#

matplot(dados$razao,m,lty=c(1,2,2,3,3),type="l",col=c(1,2,2,4,4),bty="n",main="",
        xlab="Razão molar",ylab="Viscosidade")
points(dados$razao,dados$viscosidade, pch=20)
legend('bottomleft', c("Regressao","Confianca","Predicao"),
       bty="n", cex=0.8, lty=1:3, col=c(1,2,4))
text(0.87,1, "Reta estimada: Y = 0.8781 -0.5548X", cex=0.8)
text(0.778,0.9, "R² = 0.4834", cex=0.8)
```
<br>
Temos o intervalo de predicao maior que o intervalo de confiança, os dados ficam dois foras do intervalo de confiança, um deles ficando bem na borda do intervalo, assim tendo 25% dos dados foram o IC, o que é bem ruim.

<br><br>
(c) Apresente um intervalo de predição para uma viscosidade de 0.95.
<br>


Sabendo que o intervalo é dado por,

$$IP[Y_h, 1- \alpha]: \hat Y_h \pm
    t_{(n-2, \alpha/2)}\sqrt{\left[1 + \frac 1 n + \frac{(X_i - \bar X)^2}{S_{xx}}\right]QME}$$


```{r}
n   <- nrow(dados)
Sxx <- sum(dados$razao^2) -sum(dados$razao)^2/n
QME <- sum(dados$viscosidade - y.pred)^2 / (n - 2)
EP  <- -qt(0.95, n - 2) *sqrt(
    (1 + 1/n + (dados$razao - mean(dados$razao)^2)/(Sxx))*QME
  )

IP <- matrix(c(dados$viscosidade - EP, dados$viscosidade + EP), nrow = 2,  byrow = T,
             dimnames = list('  ' = c('L.I.', 'L.S.'), ' ' = c(paste0('X', 1:nrow(dados)))))
#  byrow = T,dimnames =  list('  ' = c('Intercept', 'x'), ' ' = c('2.5%', '97.5%'))
IP
```
<br><br>

<div style="page-break-after: always;"></div>

8. Com interesse de investigar a relação linear existente entre $X$ (nível de dose (%) nutricional na ração) e $Y$ (resposta à taxa de crescimento), um experimento foi realizado considerando um Delineamento Inteiramente Casualizada (DIC) em um grupo de animais. Realize uma análise completa para os dados e conclua.

\begin{array}{c|ccccccc}
\hline
\textrm{X}            &1&1&1&1&1&2&2&2&2&2&3&3&3&3&3&4&4&4&4&4&\\
\hline
\textrm{Y}      &2&2&1&1&0.1&1&0.1&0.1&1&1&12&10&14&17&11&7&9&15&8&10&\\
\hline
\end{array}


<br><br>

Primeiramente, buscando verificar visualmente a relação linear dos dados

```{r}
x <- c(rep(1, 5), rep(2, 5), rep(3, 5), rep(4, 5))
y <- as.numeric(c(2, 2, 1, 1, 0.1, 1, 0.1, 0.1, 1, 1, 12, 10, 14, 17, 11, 7, 9, 15, 8, 10))
dados <- data.frame(x = x, y = y)

plot(x = dados$x, y = dados$y,
     xlab = 'Dose nutricional na ração (%)',
     ylab = 'Taxa de crescimento',
     main = 'Gráfico de dispersão da taxa de crescimento\n pela dose nutricional')

```
<br>
No eixo x temos uma escala discreta, enquanto no eixo y temos uma contínua, além disso a relação linear não parece muito forte.
<br><br>

Criando um modelo linear,

```{r}
modelo1 <- lm(y~x, data= dados)
summary(modelo1)
```
O modelo é a equação $-3.36 + 3.79x$, temos que apenas a variável x é significativa, sendo ao nível de 0.0001, temos um $\bar R^2 = 0.5422$. <br>

Visualizando o ajuste temos,

```{r}
plot(x = dados$x, y = dados$y,
     xlab = 'Dose nutricional na ração (%)',
     ylab = 'Taxa de crescimento',
     main = 'Gráfico de dispersão da taxa de crescimento\n pela dose nutricional')
curve(modelo1$coefficients[1] + modelo1$coefficients[2]*x, add = T, col = 'red', lty = 1, lwd = 2)
```

A reta passa nos pontos com valor 1 no x e nos pontos com valor 4 no eixo x, nos outros ela passa ao lado.
<br><br>
Agora verificando os pressupostos do modelo, <br>

<div style="page-break-after: always;"></div>

**Normalidade dos resíduos** <br>

Para isso, será usado o teste de Shapiro-Wilk, que possui as seguintes hipóteses,

$$
    \begin{cases}
H_{0}: \textrm{Os dados seguem uma distribuição normal.}\\
H_{1}: \textrm{Os dados não se distribuem como uma normal.}
    \end{cases}
$$
<br>
```{r}
shapiro.test(modelo1$residuals)
```
Assim, utilizando 95% de confiança, não rejeitamos $H_0$, temos que os resíduos seguem uma distribuição normal.
<br><br>

**Homocedastidade**

Para isso, o teste de Goldfeld-Quandt será usado, o qual possui as seguintes hipóteses,

$$
    \begin{cases}
H_{0}: \textrm{A homocedasticidade está presente.}\\
H_{1}: \textrm{A homocedasticidade não está presente.}
    \end{cases}
$$

```{r}
library(lmtest)

gqtest(modelo1)
```
Usando 95% de confiança rejeitamos $H_0$, ou seja, a homocedasticidade não está presente nas variáveis independentes.
<br><br>

**Resíduos independentes e não correlacionados**<br>
<br>


Para testar se existe de fato não correlação será usado o teste de Durbin-Watson, que possui as seguintes hipóteses,

$$
    \begin{cases}
H_{0}: \rho = 0,    &&\textrm{Existe independência}\\
H_{1}: \rho \not= 0,&&\textrm{Não existe independência}
    \end{cases}
$$

```{r}
library(car)
durbinWatsonTest(modelo1)
```
<br>
Assim, usando 95% de confiança, rejeitamos $H_0$, ou seja, não existe independência nos resíduos.
<br><br>

Assim, dois dos pressupostos furaram para esse modelo, o de independência de resíduos e o de homecedasticidade, assim o modelo não sendo confiável.

<br><br>

Com isso, se conclui que o modelo linear simples não é adequado para os dados.
<br><br>

Buscando transformar os dados para tentar aplicar novamente o modelo, será usado Box-Cox, sendo a transformação a seguinte:<br>

$$
    \begin{cases}
Y \rightarrow \frac{Y^\lambda - 1}{\lambda}&,    & \lambda \not = 0;\\
\\
Y \rightarrow \ln(Y)&,& \lambda = 0.
    \end{cases}
$$

<br><br>
O $\lambda$ que foi calculado pelo método e seu intervalo para nossos dados foi o seguinte,
```{r}
library(MASS)

w = boxcox(x ~ y, data=dados, plotit=T)

posicao = which.max(w$y)

lambda = w$x[posicao]
dados$y2 = (dados$y^(lambda)-1)/lambda
modelo2 <- lm(y2 ~ x, data = dados)
```
<br>
O $\lambda$ foi igual a 0.7070, o intervalo contém o zero, no entanto será usado lambda = 0.707 pelo baixo tamanho da amostra.

```{r}
plot(x = dados$x, y = dados$y2,
     xlab = 'Dose nutricional na ração (%)',
     ylab = 'Taxa de crescimento',
     main = 'Gráfico de dispersão da taxa de crescimento\n pela dose nutricional')
curve(modelo2$coefficients[1] + modelo2$coefficients[2]*x, add = T, col = 'red', lty = 1, lwd = 2)
```

<br>
Agora verificando os pressupostos,

```{r}
shapiro.test(modelo2$residuals)
gqtest(modelo2)
durbinWatsonTest(modelo2)
```
Agora o modelo atendeu o pressuposto de homocedasticidade, no entanto furou os de normalidade de resíduos e indepedência de resíduos.
<br><br>
Assim, mesmo transformando os dados os pressupostos não foram atendidos, ou seja, de fato o modelo linear simples não é adequado para o ajuste dos dados.
